{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project description:\n",
    "\n",
    "Online shops have exact same items that are sold by multiple people. ML could help in order to group them.\n",
    "\n",
    "In Kaggle competition [Shopee - Price Match Guarantee](https://www.kaggle.com/c/shopee-product-matching/overview) there are two item features provided: image and description.\n",
    "\n",
    "Submission file has to include up to 50 best matches from train dataset for every test item.\n",
    "\n",
    "Evaluation: mean F1\n",
    "\n",
    "-----------\n",
    "\n",
    "#### Planned approach in general\n",
    "\n",
    "1. With pre-trained NLP model extract probabilities of described items\n",
    "2. With pre-trained NN model extract probabilities of shown items\n",
    "3. Build AND gate that include both models threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\n\\nimport torch  # 1.4.0\\nimport torch.nn as nn\\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\\nimport torchvision  # 0.5.0\\n\\nimport transformers  # 3.5.1\\nfrom transformers import AutoModel, BertTokenizerFast, AdamW\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.utils.class_weight import compute_class_weight\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom imblearn.over_sampling import RandomOverSampler\\n\\nfrom functools import partial\\n\\nimport time\\n\\nimport io\\nimport os\\n\\nRANDOM = 777\\n\\n%load_ext nb_black\\n\\ndevice = torch.device(\\\"cuda\\\")\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\n\\nimport torch  # 1.4.0\\nimport torch.nn as nn\\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\\nimport torchvision  # 0.5.0\\n\\nimport transformers  # 3.5.1\\nfrom transformers import AutoModel, BertTokenizerFast, AdamW\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.utils.class_weight import compute_class_weight\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom imblearn.over_sampling import RandomOverSampler\\n\\nfrom functools import partial\\n\\nimport time\\n\\nimport io\\nimport os\\n\\nRANDOM = 777\\n\\n%load_ext nb_black\\n\\ndevice = torch.device(\\\"cuda\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch  # 1.4.0\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torchvision  # 0.5.0\n",
    "\n",
    "import transformers  # 3.5.1\n",
    "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import time\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "RANDOM = 777\n",
    "\n",
    "%load_ext nb_black\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"train_df = pd.read_csv(\\\"train.csv\\\")\\ntest_df = pd.read_csv(\\\"test.csv\\\")\";\n",
       "                var nbb_formatted_code = \"train_df = pd.read_csv(\\\"train.csv\\\")\\ntest_df = pd.read_csv(\\\"test.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \n",
       "0                          Paper Bag Victoria Secret    249114794  \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045  \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891  \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188  \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"train_df.head()\";\n",
       "                var nbb_formatted_code = \"train_df.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34250, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"train_df.shape\";\n",
       "                var nbb_formatted_code = \"train_df.shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>0006c8e5462ae52167402bac1c2e916e.jpg</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>0007585c4d0f932859339129f709bfdc.jpg</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>0008377d3662e83ef44e1881af38b879.jpg</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                 image       image_phash  \\\n",
       "0  test_2255846744  0006c8e5462ae52167402bac1c2e916e.jpg  ecc292392dc7687a   \n",
       "1  test_3588702337  0007585c4d0f932859339129f709bfdc.jpg  e9968f60d2699e2c   \n",
       "2  test_4015706929  0008377d3662e83ef44e1881af38b879.jpg  ba81c17e3581cabe   \n",
       "\n",
       "                                               title  \n",
       "0  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2   READY Lemonilo Mie instant sehat kuah dan goreng  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"test_df.head()\";\n",
       "                var nbb_formatted_code = \"test_df.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11014"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"train_df.label_group.nunique()\";\n",
       "                var nbb_formatted_code = \"train_df.label_group.nunique()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.label_group.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label_group'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"val_counts = train_df.label_group.value_counts()\\nvalue_df = pd.DataFrame(val_counts)\\nvalue_df[\\\"count\\\"] = 1\\nvalue_df.groupby(\\\"label_group\\\").sum().plot(kind=\\\"bar\\\")\";\n",
       "                var nbb_formatted_code = \"val_counts = train_df.label_group.value_counts()\\nvalue_df = pd.DataFrame(val_counts)\\nvalue_df[\\\"count\\\"] = 1\\nvalue_df.groupby(\\\"label_group\\\").sum().plot(kind=\\\"bar\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_counts = train_df.label_group.value_counts()\n",
    "value_df = pd.DataFrame(val_counts)\n",
    "value_df[\"count\"] = 1\n",
    "value_df.groupby(\"label_group\").sum().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of products have only 2 representatives of them in train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# extracting text and labels for train and test datasets\\ntrain_text = train_df.title.values\\ntest_text = test_df.title.values\\ntrain_labels = train_df.label_group.values\";\n",
       "                var nbb_formatted_code = \"# extracting text and labels for train and test datasets\\ntrain_text = train_df.title.values\\ntest_text = test_df.title.values\\ntrain_labels = train_df.label_group.values\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extracting text and labels for train and test datasets\n",
    "train_text = train_df.title.values\n",
    "test_text = test_df.title.values\n",
    "train_labels = train_df.label_group.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# label encoding\\nle = LabelEncoder()\\ntrain_labels = le.fit_transform(train_labels)\";\n",
       "                var nbb_formatted_code = \"# label encoding\\nle = LabelEncoder()\\ntrain_labels = le.fit_transform(train_labels)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# label encoding\n",
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# balancing dataset\\noversample = RandomOverSampler(sampling_strategy=\\\"not majority\\\")\\ntrain_text, train_labels = oversample.fit_resample(\\n    train_text.reshape(-1, 1), train_labels\\n)\";\n",
       "                var nbb_formatted_code = \"# balancing dataset\\noversample = RandomOverSampler(sampling_strategy=\\\"not majority\\\")\\ntrain_text, train_labels = oversample.fit_resample(\\n    train_text.reshape(-1, 1), train_labels\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# balancing dataset\n",
    "oversample = RandomOverSampler(sampling_strategy=\"not majority\")\n",
    "train_text, train_labels = oversample.fit_resample(\n",
    "    train_text.reshape(-1, 1), train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"train_text = train_text.reshape(-1)\";\n",
       "                var nbb_formatted_code = \"train_text = train_text.reshape(-1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_text = train_text.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"train_text, val_text, train_labels, val_labels = train_test_split(\\n    train_text, train_labels, random_state=RANDOM, test_size=0.5\\n)\";\n",
       "                var nbb_formatted_code = \"train_text, val_text, train_labels, val_labels = train_test_split(\\n    train_text, train_labels, random_state=RANDOM, test_size=0.5\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_text, val_text, train_labels, val_labels = train_test_split(\n",
    "    train_text, train_labels, random_state=RANDOM, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# import BERT-base pretrained model\\nbert = AutoModel.from_pretrained(\\\"bert-base-uncased\\\")\\n\\n# Load the BERT tokenizer\\ntokenizer = BertTokenizerFast.from_pretrained(\\\"bert-base-uncased\\\")\";\n",
       "                var nbb_formatted_code = \"# import BERT-base pretrained model\\nbert = AutoModel.from_pretrained(\\\"bert-base-uncased\\\")\\n\\n# Load the BERT tokenizer\\ntokenizer = BertTokenizerFast.from_pretrained(\\\"bert-base-uncased\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# get length of all the messages in the train set\\nseq_len = [len(i.split()) for i in train_text]\\n\\npd.Series(seq_len).hist(bins=30)\";\n",
       "                var nbb_formatted_code = \"# get length of all the messages in the train set\\nseq_len = [len(i.split()) for i in train_text]\\n\\npd.Series(seq_len).hist(bins=30)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20** is the optimal number for padding lenght, while majority of sentences fit into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ovidi\\anaconda3\\envs\\berta\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# tokenize and encode sequences in the training set\\ntokens_train = tokenizer.batch_encode_plus(\\n    train_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\\n)\\n\\n# tokenize and encode sequences in the validation set\\ntokens_val = tokenizer.batch_encode_plus(\\n    val_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\\n)\\n\\n# tokenize and encode sequences in the test set\\ntokens_test = tokenizer.batch_encode_plus(\\n    test_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\\n)\";\n",
       "                var nbb_formatted_code = \"# tokenize and encode sequences in the training set\\ntokens_train = tokenizer.batch_encode_plus(\\n    train_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\\n)\\n\\n# tokenize and encode sequences in the validation set\\ntokens_val = tokenizer.batch_encode_plus(\\n    val_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\\n)\\n\\n# tokenize and encode sequences in the test set\\ntokens_test = tokenizer.batch_encode_plus(\\n    test_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(), max_length=20, pad_to_max_length=True, truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"## convert lists to tensors\\n\\ntrain_seq = torch.tensor(tokens_train[\\\"input_ids\\\"])\\ntrain_mask = torch.tensor(tokens_train[\\\"attention_mask\\\"])\\ntrain_y = torch.tensor(train_labels.tolist())\\n\\nval_seq = torch.tensor(tokens_val[\\\"input_ids\\\"])\\nval_mask = torch.tensor(tokens_val[\\\"attention_mask\\\"])\\nval_y = torch.tensor(val_labels.tolist())\\n\\ntest_seq = torch.tensor(tokens_test[\\\"input_ids\\\"])\\ntest_mask = torch.tensor(tokens_test[\\\"attention_mask\\\"])\";\n",
       "                var nbb_formatted_code = \"## convert lists to tensors\\n\\ntrain_seq = torch.tensor(tokens_train[\\\"input_ids\\\"])\\ntrain_mask = torch.tensor(tokens_train[\\\"attention_mask\\\"])\\ntrain_y = torch.tensor(train_labels.tolist())\\n\\nval_seq = torch.tensor(tokens_val[\\\"input_ids\\\"])\\nval_mask = torch.tensor(tokens_val[\\\"attention_mask\\\"])\\nval_y = torch.tensor(val_labels.tolist())\\n\\ntest_seq = torch.tensor(tokens_test[\\\"input_ids\\\"])\\ntest_mask = torch.tensor(tokens_test[\\\"attention_mask\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
    "train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
    "val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
    "test_mask = torch.tensor(tokens_test[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# define a batch size\\nbatch_size = 32\\n\\n# wrap tensors\\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\\n\\n# sampler for sampling the data during training\\ntrain_sampler = RandomSampler(train_data)\\n\\n# dataLoader for train set\\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\\n\\n# wrap tensors\\nval_data = TensorDataset(val_seq, val_mask, val_y)\\n\\n# sampler for sampling the data during training\\nval_sampler = SequentialSampler(val_data)\\n\\n# dataLoader for validation set\\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\";\n",
       "                var nbb_formatted_code = \"# define a batch size\\nbatch_size = 32\\n\\n# wrap tensors\\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\\n\\n# sampler for sampling the data during training\\ntrain_sampler = RandomSampler(train_data)\\n\\n# dataLoader for train set\\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\\n\\n# wrap tensors\\nval_data = TensorDataset(val_seq, val_mask, val_y)\\n\\n# sampler for sampling the data during training\\nval_sampler = SequentialSampler(val_data)\\n\\n# dataLoader for validation set\\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# freeze all the parameters\\nfor param in bert.parameters():\\n    param.requires_grad = False\";\n",
       "                var nbb_formatted_code = \"# freeze all the parameters\\nfor param in bert.parameters():\\n    param.requires_grad = False\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"class BERT_Arch(nn.Module):\\n    def __init__(self, bert):\\n\\n        super(BERT_Arch, self).__init__()\\n\\n        self.bert = bert\\n\\n        # relu activation function\\n        self.relu = nn.ReLU()\\n\\n        # dense layer 1\\n        self.fc1 = nn.Linear(768, train_df.label_group.nunique())\\n\\n        # softmax activation function\\n        self.softmax = nn.LogSoftmax(dim=1)\\n\\n    # define the forward pass\\n    def forward(self, sent_id, mask):\\n\\n        # pass the inputs to the model\\n        _, cls_hs = self.bert(sent_id, attention_mask=mask)\\n\\n        x = self.fc1(cls_hs)\\n\\n        x = self.relu(x)\\n\\n        # apply softmax activation\\n        x = self.softmax(x)\\n\\n        return x\";\n",
       "                var nbb_formatted_code = \"class BERT_Arch(nn.Module):\\n    def __init__(self, bert):\\n\\n        super(BERT_Arch, self).__init__()\\n\\n        self.bert = bert\\n\\n        # relu activation function\\n        self.relu = nn.ReLU()\\n\\n        # dense layer 1\\n        self.fc1 = nn.Linear(768, train_df.label_group.nunique())\\n\\n        # softmax activation function\\n        self.softmax = nn.LogSoftmax(dim=1)\\n\\n    # define the forward pass\\n    def forward(self, sent_id, mask):\\n\\n        # pass the inputs to the model\\n        _, cls_hs = self.bert(sent_id, attention_mask=mask)\\n\\n        x = self.fc1(cls_hs)\\n\\n        x = self.relu(x)\\n\\n        # apply softmax activation\\n        x = self.softmax(x)\\n\\n        return x\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768, train_df.label_group.nunique())\n",
    "\n",
    "        # softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        # pass the inputs to the model\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# pass the pre-trained BERT to our define architecture\\nmodel = BERT_Arch(bert)\\n\\n# push the model to GPU\\nmodel = model.to(device)\";\n",
       "                var nbb_formatted_code = \"# pass the pre-trained BERT to our define architecture\\nmodel = BERT_Arch(bert)\\n\\n# push the model to GPU\\nmodel = model.to(device)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# define the optimizer\\noptimizer = AdamW(model.parameters(), lr=1e-3)\";\n",
       "                var nbb_formatted_code = \"# define the optimizer\\noptimizer = AdamW(model.parameters(), lr=1e-3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"# defining loss metric\\ncross_entropy = nn.CrossEntropyLoss()\";\n",
       "                var nbb_formatted_code = \"# defining loss metric\\ncross_entropy = nn.CrossEntropyLoss()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# defining loss metric\n",
    "cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"# function to train the model\\ndef train():\\n\\n    model.train()\\n\\n    total_loss, total_accuracy = 0, 0\\n\\n    # empty list to save model predictions\\n    total_preds = []\\n\\n    # iterate over batches\\n    for step, batch in enumerate(train_dataloader):\\n\\n        # progress update after every 50 batches.\\n        if step % 50 == 0 and not step == 0:\\n            print(\\\"  Batch {:>5,}  of  {:>5,}.\\\".format(step, len(train_dataloader)))\\n\\n        # push the batch to gpu\\n        batch = [r.to(device) for r in batch]\\n\\n        sent_id, mask, labels = batch\\n\\n        # clear previously calculated gradients\\n        model.zero_grad()\\n\\n        # get model predictions for the current batch\\n        preds = model(sent_id, mask)\\n\\n        # compute the loss between actual and predicted values\\n        loss = cross_entropy(preds, labels)\\n\\n        # add on to the total loss\\n        total_loss = total_loss + loss.item()\\n\\n        # backward pass to calculate the gradients\\n        loss.backward()\\n\\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n\\n        # update parameters\\n        optimizer.step()\\n\\n        # model predictions are stored on GPU. So, push it to CPU\\n        preds = preds.detach().cpu().numpy()\\n\\n        # append the model predictions\\n        total_preds.append(preds)\\n\\n    # compute the training loss of the epoch\\n    avg_loss = total_loss / len(train_dataloader)\\n\\n    # returns the loss and predictions\\n    return avg_loss, total_preds\";\n",
       "                var nbb_formatted_code = \"# function to train the model\\ndef train():\\n\\n    model.train()\\n\\n    total_loss, total_accuracy = 0, 0\\n\\n    # empty list to save model predictions\\n    total_preds = []\\n\\n    # iterate over batches\\n    for step, batch in enumerate(train_dataloader):\\n\\n        # progress update after every 50 batches.\\n        if step % 50 == 0 and not step == 0:\\n            print(\\\"  Batch {:>5,}  of  {:>5,}.\\\".format(step, len(train_dataloader)))\\n\\n        # push the batch to gpu\\n        batch = [r.to(device) for r in batch]\\n\\n        sent_id, mask, labels = batch\\n\\n        # clear previously calculated gradients\\n        model.zero_grad()\\n\\n        # get model predictions for the current batch\\n        preds = model(sent_id, mask)\\n\\n        # compute the loss between actual and predicted values\\n        loss = cross_entropy(preds, labels)\\n\\n        # add on to the total loss\\n        total_loss = total_loss + loss.item()\\n\\n        # backward pass to calculate the gradients\\n        loss.backward()\\n\\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n\\n        # update parameters\\n        optimizer.step()\\n\\n        # model predictions are stored on GPU. So, push it to CPU\\n        preds = preds.detach().cpu().numpy()\\n\\n        # append the model predictions\\n        total_preds.append(preds)\\n\\n    # compute the training loss of the epoch\\n    avg_loss = total_loss / len(train_dataloader)\\n\\n    # returns the loss and predictions\\n    return avg_loss, total_preds\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"# function for evaluating the model\\ndef evaluate():\\n\\n    print(\\\"\\\\nEvaluating...\\\")\\n\\n    # deactivate dropout layers\\n    model.eval()\\n\\n    total_loss, total_accuracy = 0, 0\\n\\n    # empty list to save the model predictions\\n    total_preds = []\\n\\n    # iterate over batches\\n    for step, batch in enumerate(val_dataloader):\\n\\n        # Progress update every 50 batches.\\n        if step % 50 == 0 and not step == 0:\\n\\n            # Report progress.\\n            print(\\\"  Batch {:>5,}  of  {:>5,}.\\\".format(step, len(val_dataloader)))\\n\\n        # push the batch to gpu\\n        batch = [t.to(device) for t in batch]\\n\\n        sent_id, mask, labels = batch\\n\\n        # deactivate autograd\\n        with torch.no_grad():\\n\\n            # model predictions\\n            preds = model(sent_id, mask)\\n\\n            # compute the validation loss between actual and predicted values\\n            loss = cross_entropy(preds, labels)\\n\\n            total_loss = total_loss + loss.item()\\n\\n            preds = preds.detach().cpu().numpy()\\n\\n            total_preds.append(preds)\\n\\n    # compute the validation loss of the epoch\\n    avg_loss = total_loss / len(val_dataloader)\\n\\n    return avg_loss, total_preds\";\n",
       "                var nbb_formatted_code = \"# function for evaluating the model\\ndef evaluate():\\n\\n    print(\\\"\\\\nEvaluating...\\\")\\n\\n    # deactivate dropout layers\\n    model.eval()\\n\\n    total_loss, total_accuracy = 0, 0\\n\\n    # empty list to save the model predictions\\n    total_preds = []\\n\\n    # iterate over batches\\n    for step, batch in enumerate(val_dataloader):\\n\\n        # Progress update every 50 batches.\\n        if step % 50 == 0 and not step == 0:\\n\\n            # Report progress.\\n            print(\\\"  Batch {:>5,}  of  {:>5,}.\\\".format(step, len(val_dataloader)))\\n\\n        # push the batch to gpu\\n        batch = [t.to(device) for t in batch]\\n\\n        sent_id, mask, labels = batch\\n\\n        # deactivate autograd\\n        with torch.no_grad():\\n\\n            # model predictions\\n            preds = model(sent_id, mask)\\n\\n            # compute the validation loss between actual and predicted values\\n            loss = cross_entropy(preds, labels)\\n\\n            total_loss = total_loss + loss.item()\\n\\n            preds = preds.detach().cpu().numpy()\\n\\n            total_preds.append(preds)\\n\\n    # compute the validation loss of the epoch\\n    avg_loss = total_loss / len(val_dataloader)\\n\\n    return avg_loss, total_preds\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Report progress.\n",
    "            print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 1\n",
      "  Batch    50  of  8,777.\n",
      "  Batch   100  of  8,777.\n",
      "  Batch   150  of  8,777.\n",
      "  Batch   200  of  8,777.\n",
      "  Batch   250  of  8,777.\n",
      "  Batch   300  of  8,777.\n",
      "  Batch   350  of  8,777.\n",
      "  Batch   400  of  8,777.\n",
      "  Batch   450  of  8,777.\n",
      "  Batch   500  of  8,777.\n",
      "  Batch   550  of  8,777.\n",
      "  Batch   600  of  8,777.\n",
      "  Batch   650  of  8,777.\n",
      "  Batch   700  of  8,777.\n",
      "  Batch   750  of  8,777.\n",
      "  Batch   800  of  8,777.\n",
      "  Batch   850  of  8,777.\n",
      "  Batch   900  of  8,777.\n",
      "  Batch   950  of  8,777.\n",
      "  Batch 1,000  of  8,777.\n",
      "  Batch 1,050  of  8,777.\n",
      "  Batch 1,100  of  8,777.\n",
      "  Batch 1,150  of  8,777.\n",
      "  Batch 1,200  of  8,777.\n",
      "  Batch 1,250  of  8,777.\n",
      "  Batch 1,300  of  8,777.\n",
      "  Batch 1,350  of  8,777.\n",
      "  Batch 1,400  of  8,777.\n",
      "  Batch 1,450  of  8,777.\n",
      "  Batch 1,500  of  8,777.\n",
      "  Batch 1,550  of  8,777.\n",
      "  Batch 1,600  of  8,777.\n",
      "  Batch 1,650  of  8,777.\n",
      "  Batch 1,700  of  8,777.\n",
      "  Batch 1,750  of  8,777.\n",
      "  Batch 1,800  of  8,777.\n",
      "  Batch 1,850  of  8,777.\n",
      "  Batch 1,900  of  8,777.\n",
      "  Batch 1,950  of  8,777.\n",
      "  Batch 2,000  of  8,777.\n",
      "  Batch 2,050  of  8,777.\n",
      "  Batch 2,100  of  8,777.\n",
      "  Batch 2,150  of  8,777.\n",
      "  Batch 2,200  of  8,777.\n",
      "  Batch 2,250  of  8,777.\n",
      "  Batch 2,300  of  8,777.\n",
      "  Batch 2,350  of  8,777.\n",
      "  Batch 2,400  of  8,777.\n",
      "  Batch 2,450  of  8,777.\n",
      "  Batch 2,500  of  8,777.\n",
      "  Batch 2,550  of  8,777.\n",
      "  Batch 2,600  of  8,777.\n",
      "  Batch 2,650  of  8,777.\n",
      "  Batch 2,700  of  8,777.\n",
      "  Batch 2,750  of  8,777.\n",
      "  Batch 2,800  of  8,777.\n",
      "  Batch 2,850  of  8,777.\n",
      "  Batch 2,900  of  8,777.\n",
      "  Batch 2,950  of  8,777.\n",
      "  Batch 3,000  of  8,777.\n",
      "  Batch 3,050  of  8,777.\n",
      "  Batch 3,100  of  8,777.\n",
      "  Batch 3,150  of  8,777.\n",
      "  Batch 3,200  of  8,777.\n",
      "  Batch 3,250  of  8,777.\n",
      "  Batch 3,300  of  8,777.\n",
      "  Batch 3,350  of  8,777.\n",
      "  Batch 3,400  of  8,777.\n",
      "  Batch 3,450  of  8,777.\n",
      "  Batch 3,500  of  8,777.\n",
      "  Batch 3,550  of  8,777.\n",
      "  Batch 3,600  of  8,777.\n",
      "  Batch 3,650  of  8,777.\n",
      "  Batch 3,700  of  8,777.\n",
      "  Batch 3,750  of  8,777.\n",
      "  Batch 3,800  of  8,777.\n",
      "  Batch 3,850  of  8,777.\n",
      "  Batch 3,900  of  8,777.\n",
      "  Batch 3,950  of  8,777.\n",
      "  Batch 4,000  of  8,777.\n",
      "  Batch 4,050  of  8,777.\n",
      "  Batch 4,100  of  8,777.\n",
      "  Batch 4,150  of  8,777.\n",
      "  Batch 4,200  of  8,777.\n",
      "  Batch 4,250  of  8,777.\n",
      "  Batch 4,300  of  8,777.\n",
      "  Batch 4,350  of  8,777.\n",
      "  Batch 4,400  of  8,777.\n",
      "  Batch 4,450  of  8,777.\n",
      "  Batch 4,500  of  8,777.\n",
      "  Batch 4,550  of  8,777.\n",
      "  Batch 4,600  of  8,777.\n",
      "  Batch 4,650  of  8,777.\n",
      "  Batch 4,700  of  8,777.\n",
      "  Batch 4,750  of  8,777.\n",
      "  Batch 4,800  of  8,777.\n",
      "  Batch 4,850  of  8,777.\n",
      "  Batch 4,900  of  8,777.\n",
      "  Batch 4,950  of  8,777.\n",
      "  Batch 5,000  of  8,777.\n",
      "  Batch 5,050  of  8,777.\n",
      "  Batch 5,100  of  8,777.\n",
      "  Batch 5,150  of  8,777.\n",
      "  Batch 5,200  of  8,777.\n",
      "  Batch 5,250  of  8,777.\n",
      "  Batch 5,300  of  8,777.\n",
      "  Batch 5,350  of  8,777.\n",
      "  Batch 5,400  of  8,777.\n",
      "  Batch 5,450  of  8,777.\n",
      "  Batch 5,500  of  8,777.\n",
      "  Batch 5,550  of  8,777.\n",
      "  Batch 5,600  of  8,777.\n",
      "  Batch 5,650  of  8,777.\n",
      "  Batch 5,700  of  8,777.\n",
      "  Batch 5,750  of  8,777.\n",
      "  Batch 5,800  of  8,777.\n",
      "  Batch 5,850  of  8,777.\n",
      "  Batch 5,900  of  8,777.\n",
      "  Batch 5,950  of  8,777.\n",
      "  Batch 6,000  of  8,777.\n",
      "  Batch 6,050  of  8,777.\n",
      "  Batch 6,100  of  8,777.\n",
      "  Batch 6,150  of  8,777.\n",
      "  Batch 6,200  of  8,777.\n",
      "  Batch 6,250  of  8,777.\n",
      "  Batch 6,300  of  8,777.\n",
      "  Batch 6,350  of  8,777.\n",
      "  Batch 6,400  of  8,777.\n",
      "  Batch 6,450  of  8,777.\n",
      "  Batch 6,500  of  8,777.\n",
      "  Batch 6,550  of  8,777.\n",
      "  Batch 6,600  of  8,777.\n",
      "  Batch 6,650  of  8,777.\n",
      "  Batch 6,700  of  8,777.\n",
      "  Batch 6,750  of  8,777.\n",
      "  Batch 6,800  of  8,777.\n",
      "  Batch 6,850  of  8,777.\n",
      "  Batch 6,900  of  8,777.\n",
      "  Batch 6,950  of  8,777.\n",
      "  Batch 7,000  of  8,777.\n",
      "  Batch 7,050  of  8,777.\n",
      "  Batch 7,100  of  8,777.\n",
      "  Batch 7,150  of  8,777.\n",
      "  Batch 7,200  of  8,777.\n",
      "  Batch 7,250  of  8,777.\n",
      "  Batch 7,300  of  8,777.\n",
      "  Batch 7,350  of  8,777.\n",
      "  Batch 7,400  of  8,777.\n",
      "  Batch 7,450  of  8,777.\n",
      "  Batch 7,500  of  8,777.\n",
      "  Batch 7,550  of  8,777.\n",
      "  Batch 7,600  of  8,777.\n",
      "  Batch 7,650  of  8,777.\n",
      "  Batch 7,700  of  8,777.\n",
      "  Batch 7,750  of  8,777.\n",
      "  Batch 7,800  of  8,777.\n",
      "  Batch 7,850  of  8,777.\n",
      "  Batch 7,900  of  8,777.\n",
      "  Batch 7,950  of  8,777.\n",
      "  Batch 8,000  of  8,777.\n",
      "  Batch 8,050  of  8,777.\n",
      "  Batch 8,100  of  8,777.\n",
      "  Batch 8,150  of  8,777.\n",
      "  Batch 8,200  of  8,777.\n",
      "  Batch 8,250  of  8,777.\n",
      "  Batch 8,300  of  8,777.\n",
      "  Batch 8,350  of  8,777.\n",
      "  Batch 8,400  of  8,777.\n",
      "  Batch 8,450  of  8,777.\n",
      "  Batch 8,500  of  8,777.\n",
      "  Batch 8,550  of  8,777.\n",
      "  Batch 8,600  of  8,777.\n",
      "  Batch 8,650  of  8,777.\n",
      "  Batch 8,700  of  8,777.\n",
      "  Batch 8,750  of  8,777.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of  8,777.\n",
      "  Batch   100  of  8,777.\n",
      "  Batch   150  of  8,777.\n",
      "  Batch   200  of  8,777.\n",
      "  Batch   250  of  8,777.\n",
      "  Batch   300  of  8,777.\n",
      "  Batch   350  of  8,777.\n",
      "  Batch   400  of  8,777.\n",
      "  Batch   450  of  8,777.\n",
      "  Batch   500  of  8,777.\n",
      "  Batch   550  of  8,777.\n",
      "  Batch   600  of  8,777.\n",
      "  Batch   650  of  8,777.\n",
      "  Batch   700  of  8,777.\n",
      "  Batch   750  of  8,777.\n",
      "  Batch   800  of  8,777.\n",
      "  Batch   850  of  8,777.\n",
      "  Batch   900  of  8,777.\n",
      "  Batch   950  of  8,777.\n",
      "  Batch 1,000  of  8,777.\n",
      "  Batch 1,050  of  8,777.\n",
      "  Batch 1,100  of  8,777.\n",
      "  Batch 1,150  of  8,777.\n",
      "  Batch 1,200  of  8,777.\n",
      "  Batch 1,250  of  8,777.\n",
      "  Batch 1,300  of  8,777.\n",
      "  Batch 1,350  of  8,777.\n",
      "  Batch 1,400  of  8,777.\n",
      "  Batch 1,450  of  8,777.\n",
      "  Batch 1,500  of  8,777.\n",
      "  Batch 1,550  of  8,777.\n",
      "  Batch 1,600  of  8,777.\n",
      "  Batch 1,650  of  8,777.\n",
      "  Batch 1,700  of  8,777.\n",
      "  Batch 1,750  of  8,777.\n",
      "  Batch 1,800  of  8,777.\n",
      "  Batch 1,850  of  8,777.\n",
      "  Batch 1,900  of  8,777.\n",
      "  Batch 1,950  of  8,777.\n",
      "  Batch 2,000  of  8,777.\n",
      "  Batch 2,050  of  8,777.\n",
      "  Batch 2,100  of  8,777.\n",
      "  Batch 2,150  of  8,777.\n",
      "  Batch 2,200  of  8,777.\n",
      "  Batch 2,250  of  8,777.\n",
      "  Batch 2,300  of  8,777.\n",
      "  Batch 2,350  of  8,777.\n",
      "  Batch 2,400  of  8,777.\n",
      "  Batch 2,450  of  8,777.\n",
      "  Batch 2,500  of  8,777.\n",
      "  Batch 2,550  of  8,777.\n",
      "  Batch 2,600  of  8,777.\n",
      "  Batch 2,650  of  8,777.\n",
      "  Batch 2,700  of  8,777.\n",
      "  Batch 2,750  of  8,777.\n",
      "  Batch 2,800  of  8,777.\n",
      "  Batch 2,850  of  8,777.\n",
      "  Batch 2,900  of  8,777.\n",
      "  Batch 2,950  of  8,777.\n",
      "  Batch 3,000  of  8,777.\n",
      "  Batch 3,050  of  8,777.\n",
      "  Batch 3,100  of  8,777.\n",
      "  Batch 3,150  of  8,777.\n",
      "  Batch 3,200  of  8,777.\n",
      "  Batch 3,250  of  8,777.\n",
      "  Batch 3,300  of  8,777.\n",
      "  Batch 3,350  of  8,777.\n",
      "  Batch 3,400  of  8,777.\n",
      "  Batch 3,450  of  8,777.\n",
      "  Batch 3,500  of  8,777.\n",
      "  Batch 3,550  of  8,777.\n",
      "  Batch 3,600  of  8,777.\n",
      "  Batch 3,650  of  8,777.\n",
      "  Batch 3,700  of  8,777.\n",
      "  Batch 3,750  of  8,777.\n",
      "  Batch 3,800  of  8,777.\n",
      "  Batch 3,850  of  8,777.\n",
      "  Batch 3,900  of  8,777.\n",
      "  Batch 3,950  of  8,777.\n",
      "  Batch 4,000  of  8,777.\n",
      "  Batch 4,050  of  8,777.\n",
      "  Batch 4,100  of  8,777.\n",
      "  Batch 4,150  of  8,777.\n",
      "  Batch 4,200  of  8,777.\n",
      "  Batch 4,250  of  8,777.\n",
      "  Batch 4,300  of  8,777.\n",
      "  Batch 4,350  of  8,777.\n",
      "  Batch 4,400  of  8,777.\n",
      "  Batch 4,450  of  8,777.\n",
      "  Batch 4,500  of  8,777.\n",
      "  Batch 4,550  of  8,777.\n",
      "  Batch 4,600  of  8,777.\n",
      "  Batch 4,650  of  8,777.\n",
      "  Batch 4,700  of  8,777.\n",
      "  Batch 4,750  of  8,777.\n",
      "  Batch 4,800  of  8,777.\n",
      "  Batch 4,850  of  8,777.\n",
      "  Batch 4,900  of  8,777.\n",
      "  Batch 4,950  of  8,777.\n",
      "  Batch 5,000  of  8,777.\n",
      "  Batch 5,050  of  8,777.\n",
      "  Batch 5,100  of  8,777.\n",
      "  Batch 5,150  of  8,777.\n",
      "  Batch 5,200  of  8,777.\n",
      "  Batch 5,250  of  8,777.\n",
      "  Batch 5,300  of  8,777.\n",
      "  Batch 5,350  of  8,777.\n",
      "  Batch 5,400  of  8,777.\n",
      "  Batch 5,450  of  8,777.\n",
      "  Batch 5,500  of  8,777.\n",
      "  Batch 5,550  of  8,777.\n",
      "  Batch 5,600  of  8,777.\n",
      "  Batch 5,650  of  8,777.\n",
      "  Batch 5,700  of  8,777.\n",
      "  Batch 5,750  of  8,777.\n",
      "  Batch 5,800  of  8,777.\n",
      "  Batch 5,850  of  8,777.\n",
      "  Batch 5,900  of  8,777.\n",
      "  Batch 5,950  of  8,777.\n",
      "  Batch 6,000  of  8,777.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1409792 bytes. Buy new RAM!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-1578a7feb600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# save the best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-fca3d37d065c>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mtotal_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1409792 bytes. Buy new RAM!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"epochs = 1\\n\\n# set initial loss to infinite\\nbest_valid_loss = float(\\\"inf\\\")\\n\\n# empty lists to store training and validation loss of each epoch\\ntrain_losses = []\\nvalid_losses = []\\n\\n# for each epoch\\nfor epoch in range(epochs):\\n\\n    print(\\\"\\\\n Epoch {:} / {:}\\\".format(epoch + 1, epochs))\\n\\n    # train model\\n    train_loss, _ = train()\\n\\n    # evaluate model\\n    valid_loss, _ = evaluate()\\n\\n    # save the best model\\n    if valid_loss < best_valid_loss:\\n        best_valid_loss = valid_loss\\n        torch.save(model.state_dict(), \\\"saved_weights.pt\\\")\\n\\n    # append training and validation loss\\n    train_losses.append(train_loss)\\n    valid_losses.append(valid_loss)\\n\\n    print(f\\\"\\\\nTraining Loss: {train_loss:.3f}\\\")\\n    print(f\\\"Validation Loss: {valid_loss:.3f}\\\")\";\n",
       "                var nbb_formatted_code = \"epochs = 1\\n\\n# set initial loss to infinite\\nbest_valid_loss = float(\\\"inf\\\")\\n\\n# empty lists to store training and validation loss of each epoch\\ntrain_losses = []\\nvalid_losses = []\\n\\n# for each epoch\\nfor epoch in range(epochs):\\n\\n    print(\\\"\\\\n Epoch {:} / {:}\\\".format(epoch + 1, epochs))\\n\\n    # train model\\n    train_loss, _ = train()\\n\\n    # evaluate model\\n    valid_loss, _ = evaluate()\\n\\n    # save the best model\\n    if valid_loss < best_valid_loss:\\n        best_valid_loss = valid_loss\\n        torch.save(model.state_dict(), \\\"saved_weights.pt\\\")\\n\\n    # append training and validation loss\\n    train_losses.append(train_loss)\\n    valid_losses.append(valid_loss)\\n\\n    print(f\\\"\\\\nTraining Loss: {train_loss:.3f}\\\")\\n    print(f\\\"Validation Loss: {valid_loss:.3f}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# for each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(\"\\n Epoch {:} / {:}\".format(epoch + 1, epochs))\n",
    "\n",
    "    # train model\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"saved_weights.pt\")\n",
    "\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f\"\\nTraining Loss: {train_loss:.3f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
